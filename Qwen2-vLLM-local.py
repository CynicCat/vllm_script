# import os
# from transformers import AutoTokenizer
# from vllm import LLM, SamplingParams

# # è®¾ç½®ç¯å¢ƒå˜é‡ï¼šæŒ‡å®šä½¿ç”¨ CPUï¼ˆå¦‚æœä½ ç”¨çš„æ˜¯ GPUï¼Œå¯åˆ é™¤è¿™è¡Œï¼‰
# # os.environ['VLLM_TARGET_DEVICE'] = 'cpu'

# # æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°ç›®å½•ï¼‰
# model_dir = './Qwen2-0.5B'

# # åˆå§‹åŒ– tokenizerï¼ˆä»æœ¬åœ°åŠ è½½ï¼‰
# tokenizer = AutoTokenizer.from_pretrained(
#     model_dir,
#     local_files_only=True,
# )

# # æ„å»º Chat æ¨¡å¼çš„ prompt
# messages = [
#     {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªèŠå¤©æœºå™¨äºº.'},
#     {'role': 'user', 'content': 'åˆ˜èŠŠæ˜¨å¤©å»çœ‹äº†å­™ç‡•å§¿çš„æ¼”å”±ä¼šï¼Œå¥¹æ˜¯ä¸æ˜¯æ¿€åŠ¨å¼€å¿ƒåˆ°ç°åœ¨ï¼Ÿ'}
# ]
# text = tokenizer.apply_chat_template(
#     messages,
#     tokenize=False,
#     add_generation_prompt=True,
# )

# # åˆå§‹åŒ– vLLM æ¨¡å‹ï¼ˆCPU æ¨ç†ï¼‰
# llm = LLM(
#     model=model_dir,
#     tensor_parallel_size=1,  # CPU ä¸ä½¿ç”¨å¼ é‡å¹¶è¡Œ
#     device='cuda',
# )

# # è®¾ç½®é‡‡æ ·å‚æ•°
# sampling_params = SamplingParams(
#     temperature=0.7,
#     top_p=0.8,
#     repetition_penalty=1.05,
#     max_tokens=512
# )

# # æ¨ç†
# outputs = llm.generate([text], sampling_params)

# # æ‰“å°ç»“æœ
# for output in outputs:
#     prompt = output.prompt
#     generated_text = output.outputs[0].text
#     print(f'Prompt æç¤ºè¯: {prompt!r}\nå¤§æ¨¡å‹æ¨ç†è¾“å‡º: {generated_text!r}')

import os
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# å¯é€‰ï¼šæŒ‡å®šä½¿ç”¨ GPUï¼ˆå¦‚æœä½ æœ‰ GPU å¹¶ä¸”å®‰è£…äº† GPU ç‰ˆ vLLM å’Œ PyTorchï¼Œå¯ä»¥çœç•¥æ­¤è¡Œï¼‰
# os.environ['VLLM_TARGET_DEVICE'] = 'cpu'

# æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°ï¼‰
model_dir = '../model/Qwen2-0.5B'

# åˆå§‹åŒ– tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)

# æ„å»ºå¤šä¸ª Chat Promptï¼ˆæ”¯æŒå¤šè½®ï¼‰
prompts = [
    [
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': 'è¯·ä»‹ç»ä¸€ä¸‹2025å¹´å­™ç‡•å§¿ä¸Šæµ·æ¼”å”±ä¼š'}
    ],
    [
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': 'è¯·åˆ†æä¸€ä¸‹ï¼šåˆ˜èŠŠæ˜¨å¤©æ™šä¸Šå»çœ‹äº†å­™ç‡•å§¿åœ¨ä¸Šæµ·çš„å°±åœ¨æ—¥è½ä»¥åæ¼”å”±ä¼šï¼Œå¥¹æ˜¯ä¸æ˜¯æ˜¨æ™šå¼€å§‹ğŸ§ ä¸€ç›´å…´å¥‹åˆ°ç°åœ¨ï¼Ÿ'}
    ]
]

# åº”ç”¨ prompt æ¨¡æ¿
texts = [
    tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)
    for p in prompts
]

# åˆå§‹åŒ– vLLM æ¨¡å‹ï¼ˆGPU ä¼šè‡ªåŠ¨ä½¿ç”¨ï¼‰
llm = LLM(
    model=model_dir,
    tensor_parallel_size=2,  # æœ‰å¤šå¡å¯ä»¥è®¾ä¸º 2ã€4ã€8...
    # device="cuda",
)

# è®¾ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05,
    max_tokens=512
)

# æ‰§è¡Œæ‰¹é‡æ¨ç†
outputs = llm.generate(texts, sampling_params)

# è¾“å‡ºæ¸…æ™°æ ¼å¼åŒ–ç»“æœ
for idx, output in enumerate(outputs):
    prompt = output.prompt
    generated_text = output.outputs[0].text.strip()

    # å°è¯•ä» prompt ä¸­è§£æå‡º system å’Œ user å†…å®¹
    system_content = ""
    user_content = ""
    try:
        segments = prompt.split("<|im_start|>")
        for seg in segments:
            if seg.startswith("system"):
                system_content = seg.replace("system", "").replace("<|im_end|>", "").strip()
            elif seg.startswith("user"):
                user_content = seg.replace("user", "").replace("<|im_end|>", "").strip()
    except Exception:
        user_content = prompt

    # è¾“å‡º
    print("=" * 60)
    print(f"ğŸ§ª Prompt #{idx+1}")
    print(f"ğŸ§  System Prompt:\n{system_content}\n")
    print(f"ğŸ™‹â€â™‚ï¸ User Input:\n{user_content}\n")
    print(f"ğŸ¤– LLM Output:\n{generated_text}")
    print("=" * 60 + "\n")
