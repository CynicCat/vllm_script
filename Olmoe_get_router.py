# -*- coding: utf-8 -*-
import os
import sys
import torch
from torch import Tensor

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” å…¨å±€é…ç½® â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
MODEL_DIR = "/home/jin/MoE/vLLM/model/OLMoE-1B-7B-0125"
TOP_K = 4

# è·¯ç”±å†å²ï¼šlist of tuples (layer_idx, [expert_ids per token])
ROUTE_HISTORY = []

# æ§åˆ¶ä½•æ—¶å¼€å§‹è®°å½•
RECORD = False

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 1) å¯¼å…¥ vLLM å¹¶å®ä¾‹åŒ–æ¨¡å‹ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
from vllm import LLM, SamplingParams

llm = LLM(
    model=MODEL_DIR,
    tensor_parallel_size=1,   # å•å¡æ¨¡å¼
)
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    repetition_penalty=1.05,
    max_tokens=256
)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 2) ç»™æ¯ä¸ª OlmoeMoE å®ä¾‹æ³¨å†Œ forward-hook â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
from vllm.model_executor.models.olmoe import OlmoeMoE
import re

def olm_hook(module: OlmoeMoE, inp, out):
    """
    1) å§‹ç»ˆä¿å­˜ flat hidden_states ä»¥ä¾¿åç»­ topk
    2) å½“ RECORD=True æ—¶ï¼ŒæŠŠæœ¬ layer çš„ top-k è®°å½•åˆ° ROUTE_HISTORY
    """
    flat: Tensor = inp[0]  # [batch_tokens, hidden_size]
    module._hook_flat_hidden = flat

    if RECORD:
        logits, _ = module.gate(flat)                  # [batch_tokens, num_experts]
        idx = torch.topk(logits, k=TOP_K, dim=-1).indices  # [batch_tokens, TOP_K]

        # ä» module.__repr__ é‡Œè§£æ layer idx
        name = module.__repr__()
        m = re.search(r"layers\.(\d+)\.mlp", name)
        layer_idx = int(m.group(1)) if m else -1

        ROUTE_HISTORY.append((layer_idx, idx.detach().cpu().tolist()))

# éå†æ‰€æœ‰å±‚ï¼Œæ³¨å†Œ hook
# llm.model å¯èƒ½æ˜¯ä¸ª wrapperï¼Œå…·ä½“è·¯å¾„å–å†³äºç‰ˆæœ¬ï¼›é€šå¸¸æ˜¯ llm.model.model
olm_model = llm.model.model
for layer in olm_model.layers:
    layer.mlp.register_forward_hook(olm_hook)


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 3) æ‹¼è£… Prompt æ–‡æœ¬ â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
convs = [
    [
        {'role':'system','content':'You are a helpful assistant.'},
        {'role':'user','content':'è¯·ä»‹ç»ä¸€ä¸‹æµ™æ±Ÿå·¥å•†å¤§å­¦ï¼Ÿ'}
    ],
    [
        {'role':'system','content':'You are a helpful assistant.'},
        {'role':'user','content':'ç°æœ‰çš„ç¼–ç¨‹è¯­è¨€æœ‰å“ªäº›ï¼Ÿ'}
    ]
]
texts = []
for conv in convs:
    s = ""
    for m in conv:
        s += f"[{m['role'].upper()}]: {m['content']}\n"
    s += "[ASSISTANT]:"
    texts.append(s)


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 4) è¿è¡Œæ¨ç†å¹¶æ”¶é›†è·¯ç”± â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# åœ¨ generate å‰å¼€å¯è®°å½•
RECORD = True
outputs = llm.generate(texts, sampling_params)
RECORD = False  # ç”Ÿæˆå®Œæ¯•åå…³é—­è®°å½•


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” 5) æ‰“å°ç”Ÿæˆç»“æœå’Œè·¯ç”± â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
for i, out in enumerate(outputs):
    # æ ¹æ®ç‰ˆæœ¬å…¼å®¹å–æ–‡æœ¬
    if hasattr(out, "text"):
        gen = out.text
    elif hasattr(out, "outputs") and out.outputs:
        gen = out.outputs[0].text
    else:
        gen = "<no text>"
    print("="*60)
    print(f"ğŸ§ª Prompt #{i+1}\n{texts[i]}\n")
    print(f"ğŸ¤– Model Output:\n{gen.strip()}\n")

print("\n" + "="*20 + " Top-4 Routing History " + "="*20)
# æŒ‰ layer åˆ†ç»„æ‰“å°
by_layer = defaultdict(list)
for layer_idx, token_list in ROUTE_HISTORY:
    by_layer[layer_idx].append(token_list)

for layer_idx in sorted(by_layer):
    print(f"Layer {layer_idx}:")
    for token_idx, experts in enumerate(by_layer[layer_idx]):
        print(f" Token {token_idx}: {experts}")
    print()